# Modular RAG System (Text, Table, Image)

## Architecture Summary

| Component           | Tool / Model Used                          |
|---------------------|-------------------------------------------|
| PDF Parsing         | unstructured.partition.pdf                |
| Text Embedding      | sentence-transformers/all-mpnet-base-v2   |
| Vector Store        | FAISS                                     |
| LLM for RAG         | ChatGroq (model="llama3-70b-8192")        |
| Image Captioning    | Salesforce/blip-image-captioning-base     |
| VQA (Image Q&A)     | Salesforce/blip-vqa-base                   |
| Chart Classifier    | StephanAkkerman/chart-recognizer          |
| Trend Detection     | OpenCV + Canny + HoughLinesP               |
| OCR                 | pytesseract                               |
| Query Classification| sentence-transformers/all-MiniLM-L6-v2 + util.cos_sim |

---

## How it Works (Step-by-Step)

### 1. Document Parsing and Preparation
- `partition_pdf()` extracts text, tables, and images from the PDF.
- Each extracted chunk is stored as a `langchain.Document` with:
  - **page_content**: Contains either the text/table content or BLIP-generated caption.
  - **metadata**: Stores origin information (e.g., `"image_chunk_0"`), and for images includes:
    - `caption`
    - `image_base64`
    - `type`: `"image"`

### 2. Text and Table RAG Pipeline (Standard)
- Text and table documents are embedded using `sentence-transformers/all-mpnet-base-v2` and stored in a FAISS vector database.
- Queries like "Summarize the report" are answered directly using this text/table index.
- Final answers are generated by the `llama3-70b-8192` model from Groq.

### 3. Image Handling: Special Logic

#### Image Captioning as Lightweight Search Anchor
- Uses `Salesforce/blip-image-captioning-base` to generate captions.
- Captions are indexed as `page_content` for fast semantic search on image-related queries.

#### Semantic Query Classification
- Uses `sentence-transformers/all-MiniLM-L6-v2` with cosine similarity (`util.cos_sim`) to detect if a user query relates to an image/chart.
- Example queries include:  
  *"What does the chart show?"*, *"Is the line going up?"*

#### If Query is About Image: Full Image Reasoning
- After semantic search finds a relevant image document:
  - **Chart Classification**: Uses `StephanAkkerman/chart-recognizer` to confirm the image is a valid chart.
  - **BLIP-VQA**: Uses `Salesforce/blip-vqa-base` to answer user questions directly from the raw image and query.
  - **OCR Number Detection**: Uses `pytesseract` to extract numbers from the image text for precise numeric details.
  - **Visual Trend Detection**: Uses OpenCV with HSV masking, Canny edge detection, and HoughLinesP to detect colored trend lines and compute slope to infer:
    - upward trend
    - downward trend
    - flat or unclear trend

---

## Why Caption Summary Alone Is Not Enough
- Captions provide a high-level summary, which is insufficient for detailed reasoning .
- The system uses captions only for fast search to locate the right image.
- Deep reasoning is performed using BLIP-VQA, OCR, and visual trend detection on the original image.
- This approach avoids hallucination and improves both efficiency and accuracy.

---

## Bottlenecks Detected and Resolved

| Challenge                                               | Fix                                                                            |
|---------------------------------------------------------|--------------------------------------------------------------------------------|
| Hallucination from LLM answering image questions using text chunks | Detect image-related intent using `all-MiniLM-L6-v2` similarity before answering. |
| Answering image queries from caption summary only        | Use caption to locate image, then run BLIP-VQA + OCR for detailed understanding. |
| Misidentification of charts                              | Use chart classifier to verify real chart before running VQA.                  |
| Inability to infer visual patterns                       | Implement custom trend detection using OpenCV, Canny edges, HoughLines, and slope. |
| Overhead on Groq LLM for image queries                   | Run Groq LLM only for text/table queries; image queries handled separately by VQA. |

---

